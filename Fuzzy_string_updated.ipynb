{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whylucify1/ABC-Fuzzy-string/blob/main/Fuzzy_string_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding in eight different fuzzy string techniques and analysis: "
      ],
      "metadata": {
        "id": "VGIJKu0uokDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Jaccard Similarity:\n",
        "def jaccard_similarity(str1, str2):\n",
        "    set1 = set(str1.split()) # It can only consider the characters, instead of the sequence, nor the frequency of words.\n",
        "    set2 = set(str2.split())\n",
        "    return len(set1.intersection(set2)) / len(set1.union(set2))"
      ],
      "metadata": {
        "id": "0w8E6OsZj_SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jaccard similarity:\n",
        "* Definition of jaccard similarity: The Jaccard similarity is one of the indicators to measure the similarity between texting and documents. The value of Jaccard similarity is between 0 and 1. The more the value closer to 1, the more similarities it will be.\n",
        "* Pros and cons of jaccard similarity:  \n",
        "** Pros: \n",
        "  1. It is easy to calculate and being defined in python coding.\n",
        "  2. It is effective in comparing sets of binary values or presence/absence data. \n",
        "  3. Robust to the imbalance in set sizes.\n",
        "** Cons: \n",
        "  1. Not sensitive to the order or frequency of elements in sets. (It cannot consider the sequence of each character, or the frequency of each word.)\n",
        "  2. It may not perform well in numerical datasets.\n",
        "  3. It can produce misleading results with very sparse sets.\n"
      ],
      "metadata": {
        "id": "l2MQj-dqli3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Levenshtein distance:\n",
        "def levenshtein_distance(str1, str2):\n",
        "    m = len(str1)\n",
        "    n = len(str2)\n",
        "    dp = [[0 for x in range(n + 1)] for x in range(m + 1)]\n",
        "\n",
        "    for i in range(m + 1):\n",
        "        for j in range(n + 1):\n",
        "            if i == 0:\n",
        "                dp[i][j] = j\n",
        "            elif j == 0:\n",
        "                dp[i][j] = i\n",
        "            elif str1[i - 1] == str2[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1]\n",
        "            else:\n",
        "                dp[i][j] = 1 + min(dp[i][j - 1], dp[i - 1][j], dp[i - 1][j - 1])\n",
        "\n",
        "    return dp[m][n]"
      ],
      "metadata": {
        "id": "EyyvYnyPkZEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Levenshtein distance:\n",
        "* Definition: Levenshtein distance is a string metric for measuring the difference between two strings. It is also known as Edit Distance, as it calculates the minimum number of operations (insertions, deletions, and substitutions) required to transform one string into another. The distance is calculated as the number of edits required, with a smaller distance indicating a closer match between the two strings.\n",
        "\n",
        "* Pros: \n",
        "\n",
        "  1. Simple and easy to understand: The algorithm is simple and straightforward, making it easy to implement and understand.\n",
        "  2. Works well with small changes: Levenshtein distance is particularly useful for tasks where small differences in the strings need to be detected.\n",
        "  3. Fast and efficient: The algorithm has a linear time complexity, making it fast and efficient for processing large datasets.\n",
        "\n",
        "* Cons: \n",
        "\n",
        "  1. Not suitable for large changes: The algorithm may not be suitable for tasks where large differences between the strings need to be detected, as it can be sensitive to changes in the strings.\n",
        "  2. Insensitive to semantic meaning: Levenshtein distance is a character-level algorithm and does not take into account the semantic meaning of the words in the strings.\n",
        "  3. Does not account for transpositions: Levenshtein distance does not account for transpositions, i.e., swapping two adjacent characters in a string. This can lead to incorrect results in certain cases."
      ],
      "metadata": {
        "id": "MK_JJjR5ksjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hamming distance:\n",
        "def hamming_distance(str1, str2):\n",
        "    if len(str1) != len(str2):\n",
        "        raise ValueError(\"Input strings must have the same length.\")\n",
        "    return sum(el1 != el2 for el1, el2 in zip(str1, str2))"
      ],
      "metadata": {
        "id": "mAUfWQp5kkjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hamming distance: \n",
        "\n",
        "* Definition: Hamming distance is a string metric for measuring the difference between two strings of equal length. It calculates the number of positions in which the characters in the strings are different. The distance is calculated as the number of positions in which the characters differ, with a smaller distance indicating a closer match between the two strings.\n",
        "\n",
        "* Pros: \n",
        "\n",
        "  1. Fast and efficient: The algorithm has a constant time complexity, making it fast and efficient for processing large datasets.\n",
        "\n",
        "  2. Suitable for binary strings: Hamming distance is particularly useful for binary strings, where the only possible characters are 0 and 1.\n",
        "\n",
        "  3. Easy to understand: The algorithm is simple and straightforward, making it easy to implement and understand.\n",
        "\n",
        "* Cons: \n",
        "\n",
        "  1. Limited to equal-length strings: The algorithm is limited to strings of equal length and cannot be used for strings of different lengths.\n",
        "\n",
        "  2. Insensitive to semantic meaning: Hamming distance is a character-level algorithm and does not take into account the semantic meaning of the words in the strings.\n",
        "\n",
        "  3. Does not account for insertion or deletion: Hamming distance does not account for insertion or deletion of characters, which can lead to incorrect results in certain cases.\n",
        "\n"
      ],
      "metadata": {
        "id": "fawncXVkwCHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Damerau Levenshtein distance:\n",
        "def damerau_levenshtein_distance(str1, str2):\n",
        "    d = {}\n",
        "    lenstr1 = len(str1)\n",
        "    lenstr2 = len(str2)\n",
        "    for i in range(-1, lenstr1 + 1):\n",
        "        d[(i, -1)] = i + 1\n",
        "    for j in range(-1, lenstr2 + 1):\n",
        "        d[(-1, j)] = j + 1\n",
        "\n",
        "    for i in range(lenstr1):\n",
        "        for j in range(lenstr2):\n",
        "            if str1[i] == str2[j]:\n",
        "                cost = 0\n",
        "            else:\n",
        "                cost = 1\n",
        "            d[(i, j)] = min(\n",
        "                d[(i - 1, j)] + 1,\n",
        "                d[(i, j - 1)] + 1,\n",
        "                d[(i - 1, j - 1)] + cost,\n",
        "            )\n",
        "            if i and j and str1[i] == str2[j - 1] and str1[i - 1] == str2[j]:\n",
        "                d[(i, j)] = min(d[(i, j)], d[i - 2, j - 2] + cost)\n",
        "\n",
        "    return d[lenstr1 - 1, lenstr2 - 1]"
      ],
      "metadata": {
        "id": "TrXf0asnk83y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Damerau Levenshtein distance: \n",
        "\n",
        "* Definition: Damerau-Levenshtein distance is a variant of the Levenshtein distance algorithm that allows for the transposition of adjacent characters in the strings. In other words, it considers the operation of swapping two adjacent characters as a single edit, rather than as two separate operations (a deletion and an insertion).\n",
        "\n",
        "* Pros: \n",
        "\n",
        "  1. Accounts for transpositions: Damerau-Levenshtein distance accounts for transpositions, making it more suitable for certain string comparison tasks where this type of change is common.\n",
        "\n",
        "  2. Fast and efficient: The algorithm has a linear time complexity, making it fast and efficient for processing large datasets.\n",
        "\n",
        "  3. Easy to understand: The algorithm is a variation of the Levenshtein distance algorithm, which is simple and straightforward, making it easy to understand and implement.\n",
        "\n",
        "* Cons: \n",
        "\n",
        "  1. More complex than Levenshtein distance: The algorithm is more complex than the standard Levenshtein distance algorithm, which can make it more difficult to implement and understand.\n",
        "\n",
        "  2. Insensitive to semantic meaning: Damerau-Levenshtein distance is a character-level algorithm and does not take into account the semantic meaning of the words in the strings.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bnPczLJpw0OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Longest common subsequence (LCS): \n",
        "def longest_common_subsequence(str1, str2):\n",
        "    m = len(str1)\n",
        "    n = len(str2)\n",
        "    dp = [[0 for j in range(n + 1)] for i in range(m + 1)]\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if str1[i - 1] == str2[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1] + 1\n",
        "            else:\n",
        "                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
        "    index = dp[m][n]\n",
        "    lcs = [\"\"] * (index + 1)\n",
        "    lcs[index] = \"\"\n",
        "    i = m\n",
        "    j = n\n",
        "    while i > 0 and j > 0:\n",
        "        if str1[i - 1] == str2[j - 1]:\n",
        "            lcs[index - 1] = str1[i - 1]\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "            index -= 1\n",
        "        elif dp[i - 1][j] > dp[i][j - 1]:\n",
        "            i -= 1\n",
        "        else:\n",
        "            j -= 1\n",
        "    return \"\".join(lcs)"
      ],
      "metadata": {
        "id": "FgVadu5jlR4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Longest common subsequence: \n",
        "\n",
        "* Definition: The Longest Common Subsequence (LCS) is a string metric for measuring the similarity between two strings. It calculates the length of the longest contiguous subsequence that is present in both strings. A subsequence is a sequence of characters that appears in the same order in the original string, but not necessarily consecutively.\n",
        "\n",
        "* Pros: \n",
        "\n",
        "  1. Accounts for semantic meaning: LCS takes into account the semantic meaning of the words in the strings, making it more suitable for certain string comparison tasks where the order of the characters is important.\n",
        "\n",
        "  2. Fast and efficient: The algorithm has a polynomial time complexity, making it fast and efficient for processing large datasets.\n",
        "\n",
        "  3. Can be extended to sequences of other data types: The LCS algorithm can be extended to sequences of other data types, such as sequences of integers or sequences of sets.\n",
        "\n",
        "* Cons: \n",
        "\n",
        "  1. Sensitive to insertion and deletion: LCS is sensitive to insertion and deletion of characters, which can lead to incorrect results in certain cases.\n",
        "\n",
        "  2. More complex than other algorithms: The LCS algorithm is more complex than other string comparison algorithms, such as Levenshtein distance or Hamming distance, which can make it more difficult to implement and understand.\n",
        "\n"
      ],
      "metadata": {
        "id": "a1t8Ir90xong"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosine similarity:\n",
        "import math\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = sum(x * y for x, y in zip(vec1, vec2))\n",
        "    magnitude_vec1 = math.sqrt(sum(x ** 2 for x in vec1))\n",
        "    magnitude_vec2 = math.sqrt(sum(x ** 2 for x in vec2))\n",
        "    return dot_product / (magnitude_vec1 * magnitude_vec2)"
      ],
      "metadata": {
        "id": "quoYtj-8mMSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cosine similarity: \n",
        "\n",
        "* Definition: Cosine Similarity is a measure of similarity between two non-zero vectors of an inner product space. It is commonly used for text similarity and information retrieval tasks, such as document classification and recommendation systems. Cosine Similarity is calculated as the cosine of the angle between the two vectors, with a value of 1 indicating that the vectors are identical and a value of 0 indicating that they are orthogonal and have no similarity.\n",
        "\n",
        "* Pros: \n",
        "\n",
        "  1. Accounts for semantic meaning: Cosine Similarity takes into account the semantic meaning of the words in the vectors, making it more suitable for certain text comparison tasks where the meaning of the words is important.\n",
        "\n",
        "  2. Fast and efficient: The algorithm has a linear time complexity, making it fast and efficient for processing large datasets.\n",
        "\n",
        "  3. Suitable for high-dimensional data: Cosine Similarity is suitable for high-dimensional data, such as text data, where the dimensionality of the vectors is large.\n",
        "\n",
        "* Cons: \n",
        "\n",
        "  1. Sensitive to vector scaling: Cosine Similarity is sensitive to vector scaling, meaning that the similarity between two vectors can be affected by changes in the magnitude of the vectors.\n",
        "\n",
        "  2. Does not account for the magnitude of the vectors: Cosine Similarity does not take into account the magnitude of the vectors, which can lead to incorrect results in certain cases."
      ],
      "metadata": {
        "id": "1VIonQ74x_bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Smith-Waterman Algorithm:\n",
        "def smith_waterman(str1, str2, match=2, mismatch=-1, gap=-1):\n",
        "    m = len(str1)\n",
        "    n = len(str2)\n",
        "    dp = [[0 for j in range(n + 1)] for i in range(m + 1)]\n",
        "    max_score = 0\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            score = max(\n",
        "                0,\n",
        "                dp[i - 1][j - 1] + (match if str1[i - 1] == str2[j - 1] else mismatch),\n",
        "                dp[i - 1][j] + gap,\n",
        "                dp[i][j - 1] + gap,\n",
        "            )\n",
        "            dp[i][j] = score\n",
        "            max_score = max(max_score, score)\n",
        "    return max_score"
      ],
      "metadata": {
        "id": "tJoNCa3dmPpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Smith-Waterman Algorithm: \n",
        "\n",
        "* Difinition: The Smith-Waterman algorithm is a local sequence alignment algorithm used to align two sequences of nucleotides or amino acids. Unlike global alignment algorithms, such as the Needleman-Wunsch algorithm, the Smith-Waterman algorithm performs a local alignment, meaning that it finds the best possible match between substrings of the two sequences.\n",
        "\n",
        "* Pros: \n",
        "\n",
        "  1. Accounts for gaps: The Smith-Waterman algorithm takes into account the presence of gaps in the sequences, making it more suitable for certain sequence comparison tasks where gaps are common.\n",
        "\n",
        "  2. High accuracy: The Smith-Waterman algorithm is known for its high accuracy in detecting local similarities between sequences, making it a popular choice in bioinformatics and molecular biology.\n",
        "\n",
        "  3. Can be used with any scoring system: The Smith-Waterman algorithm can be used with any scoring system, including custom-designed scoring matrices, which makes it flexible for a wide range of applications.\n",
        "\n",
        "* Cons: \n",
        "\n",
        "  1. Slow and computationally intensive: The Smith-Waterman algorithm has a high time complexity, making it slow and computationally intensive for processing large datasets.\n",
        "\n",
        "  2. Sensitive to scoring parameters: The Smith-Waterman algorithm is sensitive to the scoring parameters, such as gap penalties and substitution matrices, which can affect the results of the alignment.\n",
        "\n"
      ],
      "metadata": {
        "id": "XFCYU2CzzCfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ratcliff/Obershelp Algorithm:\n",
        "def find_split(A, B):\n",
        "    m = len(A)\n",
        "    n = len(B)\n",
        "    for i in range(min(m, n)):\n",
        "        if A[i] != B[i]:\n",
        "            return i\n",
        "    return min(m, n)\n",
        "\n",
        "def ratcliff_obershelp(A, B):\n",
        "    if not A or not B:\n",
        "        return 0\n",
        "    i = find_split(A, B)\n",
        "    if i == len(A) or i == len(B):\n",
        "        return i\n",
        "    return (\n",
        "        ratcliff_obershelp(A[:i], B[:i]) +\n",
        "        ratcliff_obershelp(A[i:], B[i:])\n",
        "    )"
      ],
      "metadata": {
        "id": "s-UZ6S6DmSeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ratcliff/Obershelp Algorithm: \n",
        "\n",
        "* Definition: sequences of characters and returns the similarity score between them. It uses a pattern recognition technique to identify common substrings in the two sequences and then matches them based on their relative positions.\n",
        "\n",
        "* Pros: \n",
        "\n",
        "  1. Fast and efficient: The Ratcliff/Obershelp algorithm has a fast and efficient implementation, making it suitable for processing large datasets.\n",
        "\n",
        "  2. High accuracy: The Ratcliff/Obershelp algorithm is known for its high accuracy in detecting similarities between sequences, making it a popular choice in various applications.\n",
        "\n",
        "  3. Can handle multiple alignments: The Ratcliff/Obershelp algorithm can handle multiple alignments, meaning that it can find all possible similarities between the two sequences.\n",
        "\n",
        "* Cons: \n",
        "\n",
        "  1. Sensitive to character ordering: The Ratcliff/Obershelp algorithm is sensitive to the ordering of the characters in the sequences, meaning that small changes in the ordering can lead to significant changes in the similarity score.\n",
        "\n",
        "  2. More complex than other algorithms: The Ratcliff/Obershelp algorithm is more complex than other string comparison algorithms, such as Levenshtein distance or Hamming distance, which can make it more difficult to implement and understand.\n"
      ],
      "metadata": {
        "id": "COmzEloJzeOP"
      }
    }
  ]
}